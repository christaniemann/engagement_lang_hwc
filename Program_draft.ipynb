{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Program_draft.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGWC6W/vHWLgqeaUYj4YSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christaniemann/engagement_lang_hwc/blob/main/Program_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install nltk==3.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFhcVvR_tcW3",
        "outputId": "3f815677-59f1-44bd-836d-be7e9fd6b683"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.4\n",
            "  Downloading nltk-3.4.zip (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (1.15.0)\n",
            "Collecting singledispatch\n",
            "  Downloading singledispatch-3.7.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436397 sha256=800aecde13a05ce37c55fd16bff4f0bd832744ce87e0cd949dda36dc6af5d068\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\n",
            "Successfully built nltk\n",
            "Installing collected packages: singledispatch, nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4 singledispatch-3.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YtvMwZVttEP",
        "outputId": "52de9c9b-5fe4-4473-a118-cea1ccb5d094"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.4)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python3.7/dist-packages (from nltk) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IrEH3JFqtr2Y",
        "outputId": "87c7d814-3013-4416-a746-4ca15d5a4388"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqQClE4kHg2p",
        "outputId": "ccc5c73c-7afe-4995-e460-86760ff4fe49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " All words: \n",
            " ['a', 'a', 'a', 'a', 'a', 'about', 'active', 'addressing', 'all', 'also', 'am', 'am', 'am', 'am', 'an', 'analysis', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'any', 'apa', 'are', 'assignment', 'assignment', 'assignment', 'assignment', 'at', 'at', 'bad', 'being', 'between', 'bio', 'body', 'brainstorm', 'business', 'business', 'but', 'changing', 'check', 'choice', 'choice', 'citation', 'citation', 'cited', 'clear', 'clear', 'comment', 'concise', 'conclusion', 'conclusion', 'consultant', 'could', 'cover', 'd', 'didn', 'don', 'edit', 'edits', 'english', 'english', 'enough', 'error', 'essay', 'everything', 'everything', 'everything', 'feedback', 'figure', 'fix', 'fixing', 'flow', 'flow', 'flow', 'flow', 'for', 'for', 'for', 'formatting', 'gave', 'general', 'get', 'give', 'given', 'good', 'grade', 'graduate', 'grammar', 'grammar', 'grammar', 'grammar', 'grammar', 'ha', 'have', 'he', 'help', 'help', 'help', 'help', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'idea', 'idea', 'idea', 'idea', 'if', 'in', 'in', 'in', 'introduction', 'is', 'is', 'just', 'know', 'lab', 'letter', 'like', 'like', 'like', 'like', 'look', 'look', 'm', 'm', 'm', 'make', 'make', 'make', 'make', 'make', 'make', 'making', 'me', 'me', 'me', 'me', 'memo', 'mistake', 'more', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'na', 'native', 'need', 'need', 'need', 'need', 'not', 'not', 'of', 'of', 'of', 'off', 'on', 'on', 'or', 'out', 'outside', 'over', 'over', 'page', 'paper', 'paper', 'paper', 'paragraph', 'passive', 'past', 'person', 'personal', 'persuasive', 'please', 'please', 'point', 'professor', 'professor', 'prompt', 'proofread', 'reorganize', 'report', 'resume', 'resume', 'revise', 'revising', 'revising', 'school', 'section', 'section', 'sense', 'sense', 'sentence', 'so', 'some', 'some', 'some', 'someone', 'sound', 'spanish', 'speaker', 'statement', 'statement', 'stuck', 'submit', 'suggestion', 'sure', 'sure', 'sure', 'sure', 'sure', 't', 't', 'taken', 'that', 'that', 'that', 'that', 'the', 'the', 'the', 'the', 'their', 'them', 'thesis', 'thesis', 'this', 'this', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'topic', 'transition', 'transition', 'two', 'use', 'using', 'using', 'verb', 'voice', 'wan', 'want', 'want', 'want', 'want', 'want', 'weird', 'whether', 'why', 'with', 'with', 'with', 'word', 'word', 'work', 'worried', 'worried', 'would', 'would', 'writing'] \n",
            "\n",
            "All Words Without Stopwords: \n",
            " ['active', 'addressing', 'also', 'analysis', 'apa', 'assignment', 'assignment', 'assignment', 'assignment', 'bad', 'bio', 'body', 'brainstorm', 'business', 'business', 'changing', 'check', 'choice', 'choice', 'citation', 'citation', 'cited', 'clear', 'clear', 'comment', 'concise', 'conclusion', 'conclusion', 'consultant', 'could', 'cover', 'edit', 'edits', 'english', 'english', 'enough', 'error', 'essay', 'everything', 'everything', 'everything', 'feedback', 'figure', 'fix', 'fixing', 'flow', 'flow', 'flow', 'flow', 'formatting', 'gave', 'general', 'get', 'give', 'given', 'good', 'grade', 'graduate', 'grammar', 'grammar', 'grammar', 'grammar', 'grammar', 'ha', 'help', 'help', 'help', 'help', 'idea', 'idea', 'idea', 'idea', 'introduction', 'know', 'lab', 'letter', 'like', 'like', 'like', 'like', 'look', 'look', 'make', 'make', 'make', 'make', 'make', 'make', 'making', 'memo', 'mistake', 'na', 'native', 'need', 'need', 'need', 'need', 'outside', 'page', 'paper', 'paper', 'paper', 'paragraph', 'passive', 'past', 'person', 'personal', 'persuasive', 'please', 'please', 'point', 'professor', 'professor', 'prompt', 'proofread', 'reorganize', 'report', 'resume', 'resume', 'revise', 'revising', 'revising', 'school', 'section', 'section', 'sense', 'sense', 'sentence', 'someone', 'sound', 'spanish', 'speaker', 'statement', 'statement', 'stuck', 'submit', 'suggestion', 'sure', 'sure', 'sure', 'sure', 'sure', 'taken', 'thesis', 'thesis', 'topic', 'transition', 'transition', 'two', 'use', 'using', 'using', 'verb', 'voice', 'wan', 'want', 'want', 'want', 'want', 'want', 'weird', 'whether', 'word', 'word', 'work', 'worried', 'worried', 'would', 'would', 'writing'] \n",
            "\n",
            "Matched Start Scores and Words: \n",
            " [[1, ['i', 'want', 'a', 'consultant', 'to', 'look', 'at', 'my', 'conclusion', 'and', 'analysis', 'section', 'i', 'would', 'like', 'them', 'to', 'make', 'sure', 'everything', 'make', 'sense']], [2, ['i', 'want', 'to', 'reorganize', 'my', 'paper', 'make', 'sure', 'i', 'm', 'addressing', 'the', 'prompt', 'and', 'that', 'my', 'essay', 'flow']], [3, ['transition', 'and', 'flow', 'and', 'word', 'choice']], [4, ['just', 'wan', 'na', 'fix', 'all', 'error', 'that', 'are', 'on', 'my', 'assignment']], [5, ['revising', 'the', 'body', 'paragraph', 'of', 'my', 'paper', 'introduction', 'thesis']], [1, ['i', 'need', 'some', 'help', 'with', 'transition', 'and', 'flow']], [2, ['i', 'want', 'someone', 'to', 'check', 'over', 'my', 'grammar', 'and', 'give', 'me', 'suggestion', 'for', 'edits']], [3, ['i', 'm', 'worried', 'whether', 'i', 'm', 'being', 'clear', 'in', 'my', 'thesis', 'statement']], [4, ['please', 'help', 'me', 'figure', 'out', 'why', 'my', 'professor', 'gave', 'me', 'a', 'bad', 'grade', 'on', 'this', 'assignment']], [5, ['i', 'could', 'use', 'some', 'help', 'with', 'my', 'english', 'grammar', 'a', 'i', 'am', 'not', 'a', 'native', 'speaker', 'of', 'english']], [3, ['i', 'need', 'to', 'proofread', 'and', 'edit', 'my', 'paper']], [2, ['my', 'professor', 'ha', 'given', 'me', 'some', 'feedback', 'i', 'd', 'like', 'to', 'look', 'over', 'their', 'comment', 'and', 'revise', 'the', 'section', 'he', 'didn', 't', 'like', 'i', 'also', 'need', 'help', 'with', 'my', 'conclusion', 'and', 'making', 'sure', 'everything', 'flow']], [4, ['grammar', 'and', 'word', 'choice', 'please']], [3, ['grammar']], [1, ['apa', 'citation', 'and', 'formatting', 'my', 'assignment']], [5, ['i', 'am', 'worried', 'about', 'this', 'business', 'memo', 'i', 'am', 'not', 'good', 'at', 'business', 'writing', 'and', 'i', 'need', 'to', 'make', 'sure', 'everything', 'is', 'concise', 'and', 'clear', 'so', 'that', 'i', 'don', 't', 'get', 'point', 'taken', 'off']], [2, ['fixing', 'my', 'citation', 'in', 'work', 'cited', 'page']], [4, ['i', 'would', 'like', 'to', 'brainstorm', 'idea', 'for', 'my', 'spanish', 'assignment', 'i', 'have', 'a', 'general', 'idea', 'of', 'the', 'topic', 'but', 'i', 'am', 'stuck', 'between', 'two', 'idea', 'i', 'want', 'to', 'make', 'sure', 'my', 'idea', 'make', 'sense', 'to', 'an', 'outside', 'person']], [2, ['changing', 'any', 'grammar', 'mistake', 'or', 'sentence', 'that', 'sound', 'weird']], [3, ['i', 'want', 'to', 'know', 'if', 'my', 'personal', 'statement', 'is', 'persuasive', 'enough', 'to', 'submit', 'to', 'my', 'graduate', 'school']], [4, ['using', 'more', 'active', 'verb', 'in', 'my', 'resume']], [3, ['revising', 'my', 'lab', 'report', 'for', 'bio', 'and', 'using', 'past', 'passive', 'voice']], [4, ['resume', 'and', 'cover', 'letter']]] \n",
            "\n",
            "\n",
            " \n",
            " BASIC STATISTICS \n",
            "\n",
            "Total words used: 316\n",
            "Total meaningful words used: 170 \n",
            "\n",
            "Average length of strings: 13.73913043478261\n",
            "Minimum length of string: 1\n",
            "Maximum length of string: 38\n",
            "\n",
            " Number of tokens for each word type: Counter({'i': 22, 'my': 20, 'and': 15, 'to': 13, 'make': 6, 'want': 5, 'a': 5, 'sure': 5, 'grammar': 5, 'like': 4, 'the': 4, 'that': 4, 'flow': 4, 'assignment': 4, 'need': 4, 'help': 4, 'me': 4, 'am': 4, 'idea': 4, 'everything': 3, 'paper': 3, 'm': 3, 'of': 3, 'some': 3, 'with': 3, 'for': 3, 'in': 3, 'look': 2, 'at': 2, 'conclusion': 2, 'section': 2, 'would': 2, 'sense': 2, 'transition': 2, 'word': 2, 'choice': 2, 'on': 2, 'revising': 2, 'thesis': 2, 'over': 2, 'worried': 2, 'clear': 2, 'statement': 2, 'please': 2, 'professor': 2, 'this': 2, 'english': 2, 'not': 2, 't': 2, 'citation': 2, 'business': 2, 'is': 2, 'using': 2, 'resume': 2, 'consultant': 1, 'analysis': 1, 'them': 1, 'reorganize': 1, 'addressing': 1, 'prompt': 1, 'essay': 1, 'just': 1, 'wan': 1, 'na': 1, 'fix': 1, 'all': 1, 'error': 1, 'are': 1, 'body': 1, 'paragraph': 1, 'introduction': 1, 'someone': 1, 'check': 1, 'give': 1, 'suggestion': 1, 'edits': 1, 'whether': 1, 'being': 1, 'figure': 1, 'out': 1, 'why': 1, 'gave': 1, 'bad': 1, 'grade': 1, 'could': 1, 'use': 1, 'native': 1, 'speaker': 1, 'proofread': 1, 'edit': 1, 'ha': 1, 'given': 1, 'feedback': 1, 'd': 1, 'their': 1, 'comment': 1, 'revise': 1, 'he': 1, 'didn': 1, 'also': 1, 'making': 1, 'apa': 1, 'formatting': 1, 'about': 1, 'memo': 1, 'good': 1, 'writing': 1, 'concise': 1, 'so': 1, 'don': 1, 'get': 1, 'point': 1, 'taken': 1, 'off': 1, 'fixing': 1, 'work': 1, 'cited': 1, 'page': 1, 'brainstorm': 1, 'spanish': 1, 'have': 1, 'general': 1, 'topic': 1, 'but': 1, 'stuck': 1, 'between': 1, 'two': 1, 'an': 1, 'outside': 1, 'person': 1, 'changing': 1, 'any': 1, 'mistake': 1, 'or': 1, 'sentence': 1, 'sound': 1, 'weird': 1, 'know': 1, 'if': 1, 'personal': 1, 'persuasive': 1, 'enough': 1, 'submit': 1, 'graduate': 1, 'school': 1, 'more': 1, 'active': 1, 'verb': 1, 'lab': 1, 'report': 1, 'bio': 1, 'past': 1, 'passive': 1, 'voice': 1, 'cover': 1, 'letter': 1}) \n",
            "\n",
            "Number of tokens for each word type (no stopwords): Counter({'make': 6, 'want': 5, 'sure': 5, 'grammar': 5, 'like': 4, 'flow': 4, 'assignment': 4, 'need': 4, 'help': 4, 'idea': 4, 'everything': 3, 'paper': 3, 'look': 2, 'conclusion': 2, 'section': 2, 'would': 2, 'sense': 2, 'transition': 2, 'word': 2, 'choice': 2, 'revising': 2, 'thesis': 2, 'worried': 2, 'clear': 2, 'statement': 2, 'please': 2, 'professor': 2, 'english': 2, 'citation': 2, 'business': 2, 'using': 2, 'resume': 2, 'consultant': 1, 'analysis': 1, 'reorganize': 1, 'addressing': 1, 'prompt': 1, 'essay': 1, 'wan': 1, 'na': 1, 'fix': 1, 'error': 1, 'body': 1, 'paragraph': 1, 'introduction': 1, 'someone': 1, 'check': 1, 'give': 1, 'suggestion': 1, 'edits': 1, 'whether': 1, 'figure': 1, 'gave': 1, 'bad': 1, 'grade': 1, 'could': 1, 'use': 1, 'native': 1, 'speaker': 1, 'proofread': 1, 'edit': 1, 'ha': 1, 'given': 1, 'feedback': 1, 'comment': 1, 'revise': 1, 'also': 1, 'making': 1, 'apa': 1, 'formatting': 1, 'memo': 1, 'good': 1, 'writing': 1, 'concise': 1, 'get': 1, 'point': 1, 'taken': 1, 'fixing': 1, 'work': 1, 'cited': 1, 'page': 1, 'brainstorm': 1, 'spanish': 1, 'general': 1, 'topic': 1, 'stuck': 1, 'two': 1, 'outside': 1, 'person': 1, 'changing': 1, 'mistake': 1, 'sentence': 1, 'sound': 1, 'weird': 1, 'know': 1, 'personal': 1, 'persuasive': 1, 'enough': 1, 'submit': 1, 'graduate': 1, 'school': 1, 'active': 1, 'verb': 1, 'lab': 1, 'report': 1, 'bio': 1, 'past': 1, 'passive': 1, 'voice': 1, 'cover': 1, 'letter': 1}) \n",
            "\n",
            "Not Engaged: [[1, ['i', 'want', 'a', 'consultant', 'to', 'look', 'at', 'my', 'conclusion', 'and', 'analysis', 'section', 'i', 'would', 'like', 'them', 'to', 'make', 'sure', 'everything', 'make', 'sense']], [1, ['i', 'need', 'some', 'help', 'with', 'transition', 'and', 'flow']], [1, ['apa', 'citation', 'and', 'formatting', 'my', 'assignment']]]\n",
            "Less Engaged: [[2, ['i', 'want', 'to', 'reorganize', 'my', 'paper', 'make', 'sure', 'i', 'm', 'addressing', 'the', 'prompt', 'and', 'that', 'my', 'essay', 'flow']], [2, ['i', 'want', 'someone', 'to', 'check', 'over', 'my', 'grammar', 'and', 'give', 'me', 'suggestion', 'for', 'edits']], [2, ['my', 'professor', 'ha', 'given', 'me', 'some', 'feedback', 'i', 'd', 'like', 'to', 'look', 'over', 'their', 'comment', 'and', 'revise', 'the', 'section', 'he', 'didn', 't', 'like', 'i', 'also', 'need', 'help', 'with', 'my', 'conclusion', 'and', 'making', 'sure', 'everything', 'flow']], [2, ['fixing', 'my', 'citation', 'in', 'work', 'cited', 'page']], [2, ['changing', 'any', 'grammar', 'mistake', 'or', 'sentence', 'that', 'sound', 'weird']]]\n",
            "Neutral: [[3, ['transition', 'and', 'flow', 'and', 'word', 'choice']], [3, ['i', 'm', 'worried', 'whether', 'i', 'm', 'being', 'clear', 'in', 'my', 'thesis', 'statement']], [3, ['i', 'need', 'to', 'proofread', 'and', 'edit', 'my', 'paper']], [3, ['grammar']], [3, ['i', 'want', 'to', 'know', 'if', 'my', 'personal', 'statement', 'is', 'persuasive', 'enough', 'to', 'submit', 'to', 'my', 'graduate', 'school']], [3, ['revising', 'my', 'lab', 'report', 'for', 'bio', 'and', 'using', 'past', 'passive', 'voice']]]\n",
            "Engaged: [[4, ['just', 'wan', 'na', 'fix', 'all', 'error', 'that', 'are', 'on', 'my', 'assignment']], [4, ['please', 'help', 'me', 'figure', 'out', 'why', 'my', 'professor', 'gave', 'me', 'a', 'bad', 'grade', 'on', 'this', 'assignment']], [4, ['grammar', 'and', 'word', 'choice', 'please']], [4, ['i', 'would', 'like', 'to', 'brainstorm', 'idea', 'for', 'my', 'spanish', 'assignment', 'i', 'have', 'a', 'general', 'idea', 'of', 'the', 'topic', 'but', 'i', 'am', 'stuck', 'between', 'two', 'idea', 'i', 'want', 'to', 'make', 'sure', 'my', 'idea', 'make', 'sense', 'to', 'an', 'outside', 'person']], [4, ['using', 'more', 'active', 'verb', 'in', 'my', 'resume']], [4, ['resume', 'and', 'cover', 'letter']]]\n",
            "Very Engaged: [[5, ['revising', 'the', 'body', 'paragraph', 'of', 'my', 'paper', 'introduction', 'thesis']], [5, ['i', 'could', 'use', 'some', 'help', 'with', 'my', 'english', 'grammar', 'a', 'i', 'am', 'not', 'a', 'native', 'speaker', 'of', 'english']], [5, ['i', 'am', 'worried', 'about', 'this', 'business', 'memo', 'i', 'am', 'not', 'good', 'at', 'business', 'writing', 'and', 'i', 'need', 'to', 'make', 'sure', 'everything', 'is', 'concise', 'and', 'clear', 'so', 'that', 'i', 'don', 't', 'get', 'point', 'taken', 'off']]] \n",
            "\n",
            "Average Length of String for Not Engaged: 12.0\n",
            "Average Length of String for Less Engaged: 16.6\n",
            "Average Length of String for Neutral: 9.166666666666666\n",
            "Average Length of String for Engaged: 13.5\n",
            "Average Length of String for Very Engaged: 20.333333333333332\n",
            "\n",
            " \n",
            " Sentiment Statistics \n",
            "\n",
            "Not engaged sentiments: \n",
            "\n",
            "Negative: 0.0\n",
            "Neutral: 0.8263333333333334\n",
            "Positive: 0.17366666666666666\n",
            "Compound: 0.34226666666666666 \n",
            " \n",
            "\n",
            "Less engaged sentiments: \n",
            "\n",
            "Negative: 0.0736\n",
            "Neutral: 0.797\n",
            "Positive: 0.1292\n",
            "Compound: 0.16807999999999998 \n",
            " \n",
            "\n",
            "Neutral sentiments: \n",
            "\n",
            "Negative: 0.028666666666666663\n",
            "Neutral: 0.9015\n",
            "Positive: 0.06983333333333334\n",
            "Compound: 0.06369999999999999 \n",
            " \n",
            "\n",
            "Engaged sentiments: \n",
            "\n",
            "Negative: 0.07216666666666667\n",
            "Neutral: 0.7488333333333334\n",
            "Positive: 0.17916666666666667\n",
            "Compound: 0.15928333333333333 \n",
            " \n",
            "\n",
            "Very engaged sentiments: \n",
            "\n",
            "Negative: 0.04\n",
            "Neutral: 0.872\n",
            "Positive: 0.08800000000000001\n",
            "Compound: 0.15919999999999998 \n",
            " \n",
            "\n",
            "\n",
            " \n",
            " \n",
            " SINGLE WORD CATEGORY ANALYSIS \n",
            "\n",
            "Not Engaged:\n",
            "['analysis', 'apa', 'assignment', 'citation', 'conclusion', 'consultant', 'everything', 'flow', 'formatting', 'help', 'like', 'look', 'make', 'make', 'need', 'section', 'sense', 'sure', 'transition', 'want', 'would'] \n",
            "\n",
            "Counter({'make': 2, 'want': 1, 'consultant': 1, 'look': 1, 'conclusion': 1, 'analysis': 1, 'section': 1, 'would': 1, 'like': 1, 'sure': 1, 'everything': 1, 'sense': 1, 'need': 1, 'help': 1, 'transition': 1, 'flow': 1, 'apa': 1, 'citation': 1, 'formatting': 1, 'assignment': 1}) \n",
            "\n",
            "Checkbox Categories:\n",
            "Counter({'transition': 1, 'flow': 1, 'citation': 1}) \n",
            "\n",
            "Verbs:\n",
            "Counter({'help': 1}) \n",
            "\n",
            "Pronouns:\n",
            "Counter({'i': 3}) \n",
            " \n",
            "\n",
            "Less Engaged:\n",
            "['addressing', 'also', 'changing', 'check', 'citation', 'cited', 'comment', 'conclusion', 'edits', 'essay', 'everything', 'feedback', 'fixing', 'flow', 'flow', 'give', 'given', 'grammar', 'grammar', 'ha', 'help', 'like', 'like', 'look', 'make', 'making', 'mistake', 'need', 'page', 'paper', 'professor', 'prompt', 'reorganize', 'revise', 'section', 'sentence', 'someone', 'sound', 'suggestion', 'sure', 'sure', 'want', 'want', 'weird', 'work'] \n",
            "\n",
            "Counter({'want': 2, 'sure': 2, 'flow': 2, 'grammar': 2, 'like': 2, 'reorganize': 1, 'paper': 1, 'make': 1, 'addressing': 1, 'prompt': 1, 'essay': 1, 'someone': 1, 'check': 1, 'give': 1, 'suggestion': 1, 'edits': 1, 'professor': 1, 'ha': 1, 'given': 1, 'feedback': 1, 'look': 1, 'comment': 1, 'revise': 1, 'section': 1, 'also': 1, 'need': 1, 'help': 1, 'conclusion': 1, 'making': 1, 'everything': 1, 'fixing': 1, 'citation': 1, 'work': 1, 'cited': 1, 'page': 1, 'changing': 1, 'mistake': 1, 'sentence': 1, 'sound': 1, 'weird': 1}) \n",
            "\n",
            "Checkbox Categories:\n",
            "Counter({'flow': 1, 'grammar': 1, 'citation': 1}) \n",
            "\n",
            "Verbs:\n",
            "Counter({'help': 1, 'check': 1, 'revise': 1, 'mistake': 1}) \n",
            "\n",
            "Pronouns:\n",
            "Counter({'i': 5}) \n",
            " \n",
            "\n",
            "Neutral:\n",
            "['bio', 'choice', 'clear', 'edit', 'enough', 'flow', 'graduate', 'grammar', 'know', 'lab', 'need', 'paper', 'passive', 'past', 'personal', 'persuasive', 'proofread', 'report', 'revising', 'school', 'statement', 'statement', 'submit', 'thesis', 'transition', 'using', 'voice', 'want', 'whether', 'word', 'worried'] \n",
            "\n",
            "Counter({'statement': 2, 'transition': 1, 'flow': 1, 'word': 1, 'choice': 1, 'worried': 1, 'whether': 1, 'clear': 1, 'thesis': 1, 'need': 1, 'proofread': 1, 'edit': 1, 'paper': 1, 'grammar': 1, 'want': 1, 'know': 1, 'personal': 1, 'persuasive': 1, 'enough': 1, 'submit': 1, 'graduate': 1, 'school': 1, 'revising': 1, 'lab': 1, 'report': 1, 'bio': 1, 'using': 1, 'past': 1, 'passive': 1, 'voice': 1}) \n",
            "\n",
            "Checkbox Categories:\n",
            "Counter({'transition': 1, 'flow': 1, 'grammar': 1}) \n",
            "\n",
            "Verbs:\n",
            "Counter({'edit': 1, 'proofread': 1}) \n",
            "\n",
            "Pronouns:\n",
            "Counter({'i': 4}) \n",
            " \n",
            "\n",
            "Engaged:\n",
            "['active', 'assignment', 'assignment', 'assignment', 'bad', 'brainstorm', 'choice', 'cover', 'error', 'figure', 'fix', 'gave', 'general', 'grade', 'grammar', 'help', 'idea', 'idea', 'idea', 'idea', 'letter', 'like', 'make', 'make', 'na', 'outside', 'person', 'please', 'please', 'professor', 'resume', 'resume', 'sense', 'spanish', 'stuck', 'sure', 'topic', 'two', 'using', 'verb', 'wan', 'want', 'word', 'would'] \n",
            "\n",
            "Counter({'idea': 4, 'assignment': 3, 'please': 2, 'make': 2, 'resume': 2, 'wan': 1, 'na': 1, 'fix': 1, 'error': 1, 'help': 1, 'figure': 1, 'professor': 1, 'gave': 1, 'bad': 1, 'grade': 1, 'grammar': 1, 'word': 1, 'choice': 1, 'would': 1, 'like': 1, 'brainstorm': 1, 'spanish': 1, 'general': 1, 'topic': 1, 'stuck': 1, 'two': 1, 'want': 1, 'sure': 1, 'sense': 1, 'outside': 1, 'person': 1, 'using': 1, 'active': 1, 'verb': 1, 'cover': 1, 'letter': 1}) \n",
            "\n",
            "Checkbox Categories:\n",
            "Counter({'idea': 1, 'grammar': 1}) \n",
            "\n",
            "Verbs:\n",
            "Counter({'help': 1, 'fix': 1, 'error': 1, 'brainstorm': 1}) \n",
            "\n",
            "Pronouns:\n",
            "Counter({'i': 4}) \n",
            " \n",
            "\n",
            "Very Engaged:\n",
            "['body', 'business', 'business', 'clear', 'concise', 'could', 'english', 'english', 'everything', 'get', 'good', 'grammar', 'help', 'introduction', 'make', 'memo', 'native', 'need', 'paper', 'paragraph', 'point', 'revising', 'speaker', 'sure', 'taken', 'thesis', 'use', 'worried', 'writing'] \n",
            "\n",
            "Counter({'english': 2, 'business': 2, 'revising': 1, 'body': 1, 'paragraph': 1, 'paper': 1, 'introduction': 1, 'thesis': 1, 'could': 1, 'use': 1, 'help': 1, 'grammar': 1, 'native': 1, 'speaker': 1, 'worried': 1, 'memo': 1, 'good': 1, 'writing': 1, 'need': 1, 'make': 1, 'sure': 1, 'everything': 1, 'concise': 1, 'clear': 1, 'get': 1, 'point': 1, 'taken': 1}) \n",
            "\n",
            "Checkbox Categories:\n",
            "Counter({'grammar': 1}) \n",
            "\n",
            "Verbs:\n",
            "Counter({'help': 1}) \n",
            "\n",
            "Pronouns:\n",
            "Counter({'i': 6}) \n",
            " \n",
            "\n",
            "\n",
            " PHRASAL (N-GRAM) CATEGORY ANALYSIS \n",
            "\n",
            "Not Engaged: \n",
            "\n",
            "Organic Phrases:\n",
            "bigrams: Counter({'i want': 1, 'want a': 1, 'a consultant': 1, 'consultant to': 1, 'to look': 1, 'look at': 1, 'at my': 1, 'my conclusion': 1, 'conclusion and': 1, 'and analysis': 1, 'analysis section': 1, 'section i': 1, 'i would': 1, 'would like': 1, 'like them': 1, 'them to': 1, 'to make': 1, 'make sure': 1, 'sure everything': 1, 'everything make': 1, 'make sense': 1, 'i need': 1, 'need some': 1, 'some help': 1, 'help with': 1, 'with transition': 1, 'transition and': 1, 'and flow': 1, 'apa citation': 1, 'citation and': 1, 'and formatting': 1, 'formatting my': 1, 'my assignment': 1})\n",
            "trigrams:  Counter({'i want a': 1, 'want a consultant': 1, 'a consultant to': 1, 'consultant to look': 1, 'to look at': 1, 'look at my': 1, 'at my conclusion': 1, 'my conclusion and': 1, 'conclusion and analysis': 1, 'and analysis section': 1, 'analysis section i': 1, 'section i would': 1, 'i would like': 1, 'would like them': 1, 'like them to': 1, 'them to make': 1, 'to make sure': 1, 'make sure everything': 1, 'sure everything make': 1, 'everything make sense': 1, 'i need some': 1, 'need some help': 1, 'some help with': 1, 'help with transition': 1, 'with transition and': 1, 'transition and flow': 1, 'apa citation and': 1, 'citation and formatting': 1, 'and formatting my': 1, 'formatting my assignment': 1}) \n",
            "\n",
            "Pre-identified phrases: \n",
            "Counter({'i want': 1, 'make sure': 1, 'make sense': 1}) \n",
            " \n",
            "\n",
            "Less Engaged: \n",
            "\n",
            "Organic Phrases:\n",
            "bigrams: Counter({'i want': 2, 'want to': 1, 'to reorganize': 1, 'reorganize my': 1, 'my paper': 1, 'paper make': 1, 'make sure': 1, 'sure i': 1, 'i m': 1, 'm addressing': 1, 'addressing the': 1, 'the prompt': 1, 'prompt and': 1, 'and that': 1, 'that my': 1, 'my essay': 1, 'essay flow': 1, 'want someone': 1, 'someone to': 1, 'to check': 1, 'check over': 1, 'over my': 1, 'my grammar': 1, 'grammar and': 1, 'and give': 1, 'give me': 1, 'me suggestion': 1, 'suggestion for': 1, 'for edits': 1, 'my professor': 1, 'professor ha': 1, 'ha given': 1, 'given me': 1, 'me some': 1, 'some feedback': 1, 'feedback i': 1, 'i d': 1, 'd like': 1, 'like to': 1, 'to look': 1, 'look over': 1, 'over their': 1, 'their comment': 1, 'comment and': 1, 'and revise': 1, 'revise the': 1, 'the section': 1, 'section he': 1, 'he didn': 1, 'didn t': 1, 't like': 1, 'like i': 1, 'i also': 1, 'also need': 1, 'need help': 1, 'help with': 1, 'with my': 1, 'my conclusion': 1, 'conclusion and': 1, 'and making': 1, 'making sure': 1, 'sure everything': 1, 'everything flow': 1, 'fixing my': 1, 'my citation': 1, 'citation in': 1, 'in work': 1, 'work cited': 1, 'cited page': 1, 'changing any': 1, 'any grammar': 1, 'grammar mistake': 1, 'mistake or': 1, 'or sentence': 1, 'sentence that': 1, 'that sound': 1, 'sound weird': 1})\n",
            "trigrams:  Counter({'i want to': 1, 'want to reorganize': 1, 'to reorganize my': 1, 'reorganize my paper': 1, 'my paper make': 1, 'paper make sure': 1, 'make sure i': 1, 'sure i m': 1, 'i m addressing': 1, 'm addressing the': 1, 'addressing the prompt': 1, 'the prompt and': 1, 'prompt and that': 1, 'and that my': 1, 'that my essay': 1, 'my essay flow': 1, 'i want someone': 1, 'want someone to': 1, 'someone to check': 1, 'to check over': 1, 'check over my': 1, 'over my grammar': 1, 'my grammar and': 1, 'grammar and give': 1, 'and give me': 1, 'give me suggestion': 1, 'me suggestion for': 1, 'suggestion for edits': 1, 'my professor ha': 1, 'professor ha given': 1, 'ha given me': 1, 'given me some': 1, 'me some feedback': 1, 'some feedback i': 1, 'feedback i d': 1, 'i d like': 1, 'd like to': 1, 'like to look': 1, 'to look over': 1, 'look over their': 1, 'over their comment': 1, 'their comment and': 1, 'comment and revise': 1, 'and revise the': 1, 'revise the section': 1, 'the section he': 1, 'section he didn': 1, 'he didn t': 1, 'didn t like': 1, 't like i': 1, 'like i also': 1, 'i also need': 1, 'also need help': 1, 'need help with': 1, 'help with my': 1, 'with my conclusion': 1, 'my conclusion and': 1, 'conclusion and making': 1, 'and making sure': 1, 'making sure everything': 1, 'sure everything flow': 1, 'fixing my citation': 1, 'my citation in': 1, 'citation in work': 1, 'in work cited': 1, 'work cited page': 1, 'changing any grammar': 1, 'any grammar mistake': 1, 'grammar mistake or': 1, 'mistake or sentence': 1, 'or sentence that': 1, 'sentence that sound': 1, 'that sound weird': 1}) \n",
            "\n",
            "Pre-identified phrases: \n",
            "Counter({'i want': 2, 'make sure': 1, 'look over': 1}) \n",
            " \n",
            "\n",
            "Neutral: \n",
            "\n",
            "Organic Phrases:\n",
            "bigrams: Counter({'i m': 2, 'transition and': 1, 'and flow': 1, 'flow and': 1, 'and word': 1, 'word choice': 1, 'm worried': 1, 'worried whether': 1, 'whether i': 1, 'm being': 1, 'being clear': 1, 'clear in': 1, 'in my': 1, 'my thesis': 1, 'thesis statement': 1, 'i need': 1, 'need to': 1, 'to proofread': 1, 'proofread and': 1, 'and edit': 1, 'edit my': 1, 'my paper': 1, 'i want': 1, 'want to': 1, 'to know': 1, 'know if': 1, 'if my': 1, 'my personal': 1, 'personal statement': 1, 'statement is': 1, 'is persuasive': 1, 'persuasive enough': 1, 'enough to': 1, 'to submit': 1, 'submit to': 1, 'to my': 1, 'my graduate': 1, 'graduate school': 1, 'revising my': 1, 'my lab': 1, 'lab report': 1, 'report for': 1, 'for bio': 1, 'bio and': 1, 'and using': 1, 'using past': 1, 'past passive': 1, 'passive voice': 1})\n",
            "trigrams:  Counter({'transition and flow': 1, 'and flow and': 1, 'flow and word': 1, 'and word choice': 1, 'i m worried': 1, 'm worried whether': 1, 'worried whether i': 1, 'whether i m': 1, 'i m being': 1, 'm being clear': 1, 'being clear in': 1, 'clear in my': 1, 'in my thesis': 1, 'my thesis statement': 1, 'i need to': 1, 'need to proofread': 1, 'to proofread and': 1, 'proofread and edit': 1, 'and edit my': 1, 'edit my paper': 1, 'i want to': 1, 'want to know': 1, 'to know if': 1, 'know if my': 1, 'if my personal': 1, 'my personal statement': 1, 'personal statement is': 1, 'statement is persuasive': 1, 'is persuasive enough': 1, 'persuasive enough to': 1, 'enough to submit': 1, 'to submit to': 1, 'submit to my': 1, 'to my graduate': 1, 'my graduate school': 1, 'revising my lab': 1, 'my lab report': 1, 'lab report for': 1, 'report for bio': 1, 'for bio and': 1, 'bio and using': 1, 'and using past': 1, 'using past passive': 1, 'past passive voice': 1}) \n",
            "\n",
            "Pre-identified phrases: \n",
            "Counter({'word choice': 1, 'i want': 1}) \n",
            " \n",
            "\n",
            "Engaged: \n",
            "\n",
            "Organic Phrases:\n",
            "bigrams: Counter({'just wan': 1, 'wan na': 1, 'na fix': 1, 'fix all': 1, 'all error': 1, 'error that': 1, 'that are': 1, 'are on': 1, 'on my': 1, 'my assignment': 1, 'please help': 1, 'help me': 1, 'me figure': 1, 'figure out': 1, 'out why': 1, 'why my': 1, 'my professor': 1, 'professor gave': 1, 'gave me': 1, 'me a': 1, 'a bad': 1, 'bad grade': 1, 'grade on': 1, 'on this': 1, 'this assignment': 1, 'grammar and': 1, 'and word': 1, 'word choice': 1, 'choice please': 1, 'i would': 1, 'would like': 1, 'like to': 1, 'to brainstorm': 1, 'brainstorm idea': 1, 'idea for': 1, 'for my': 1, 'my spanish': 1, 'spanish assignment': 1, 'assignment i': 1, 'i have': 1, 'have a': 1, 'a general': 1, 'general idea': 1, 'idea of': 1, 'of the': 1, 'the topic': 1, 'topic but': 1, 'but i': 1, 'i am': 1, 'am stuck': 1, 'stuck between': 1, 'between two': 1, 'two idea': 1, 'idea i': 1, 'i want': 1, 'want to': 1, 'to make': 1, 'make sure': 1, 'sure my': 1, 'my idea': 1, 'idea make': 1, 'make sense': 1, 'sense to': 1, 'to an': 1, 'an outside': 1, 'outside person': 1, 'using more': 1, 'more active': 1, 'active verb': 1, 'verb in': 1, 'in my': 1, 'my resume': 1, 'resume and': 1, 'and cover': 1, 'cover letter': 1})\n",
            "trigrams:  Counter({'just wan na': 1, 'wan na fix': 1, 'na fix all': 1, 'fix all error': 1, 'all error that': 1, 'error that are': 1, 'that are on': 1, 'are on my': 1, 'on my assignment': 1, 'please help me': 1, 'help me figure': 1, 'me figure out': 1, 'figure out why': 1, 'out why my': 1, 'why my professor': 1, 'my professor gave': 1, 'professor gave me': 1, 'gave me a': 1, 'me a bad': 1, 'a bad grade': 1, 'bad grade on': 1, 'grade on this': 1, 'on this assignment': 1, 'grammar and word': 1, 'and word choice': 1, 'word choice please': 1, 'i would like': 1, 'would like to': 1, 'like to brainstorm': 1, 'to brainstorm idea': 1, 'brainstorm idea for': 1, 'idea for my': 1, 'for my spanish': 1, 'my spanish assignment': 1, 'spanish assignment i': 1, 'assignment i have': 1, 'i have a': 1, 'have a general': 1, 'a general idea': 1, 'general idea of': 1, 'idea of the': 1, 'of the topic': 1, 'the topic but': 1, 'topic but i': 1, 'but i am': 1, 'i am stuck': 1, 'am stuck between': 1, 'stuck between two': 1, 'between two idea': 1, 'two idea i': 1, 'idea i want': 1, 'i want to': 1, 'want to make': 1, 'to make sure': 1, 'make sure my': 1, 'sure my idea': 1, 'my idea make': 1, 'idea make sense': 1, 'make sense to': 1, 'sense to an': 1, 'to an outside': 1, 'an outside person': 1, 'using more active': 1, 'more active verb': 1, 'active verb in': 1, 'verb in my': 1, 'in my resume': 1, 'resume and cover': 1, 'and cover letter': 1}) \n",
            "\n",
            "Pre-identified phrases: \n",
            "Counter({'just wan': 1, 'word choice': 1, 'i want': 1, 'make sure': 1, 'make sense': 1}) \n",
            " \n",
            "\n",
            "Very Engaged: \n",
            "\n",
            "Organic Phrases:\n",
            "bigrams: Counter({'i am': 3, 'am not': 2, 'revising the': 1, 'the body': 1, 'body paragraph': 1, 'paragraph of': 1, 'of my': 1, 'my paper': 1, 'paper introduction': 1, 'introduction thesis': 1, 'i could': 1, 'could use': 1, 'use some': 1, 'some help': 1, 'help with': 1, 'with my': 1, 'my english': 1, 'english grammar': 1, 'grammar a': 1, 'a i': 1, 'not a': 1, 'a native': 1, 'native speaker': 1, 'speaker of': 1, 'of english': 1, 'am worried': 1, 'worried about': 1, 'about this': 1, 'this business': 1, 'business memo': 1, 'memo i': 1, 'not good': 1, 'good at': 1, 'at business': 1, 'business writing': 1, 'writing and': 1, 'and i': 1, 'i need': 1, 'need to': 1, 'to make': 1, 'make sure': 1, 'sure everything': 1, 'everything is': 1, 'is concise': 1, 'concise and': 1, 'and clear': 1, 'clear so': 1, 'so that': 1, 'that i': 1, 'i don': 1, 'don t': 1, 't get': 1, 'get point': 1, 'point taken': 1, 'taken off': 1})\n",
            "trigrams:  Counter({'i am not': 2, 'revising the body': 1, 'the body paragraph': 1, 'body paragraph of': 1, 'paragraph of my': 1, 'of my paper': 1, 'my paper introduction': 1, 'paper introduction thesis': 1, 'i could use': 1, 'could use some': 1, 'use some help': 1, 'some help with': 1, 'help with my': 1, 'with my english': 1, 'my english grammar': 1, 'english grammar a': 1, 'grammar a i': 1, 'a i am': 1, 'am not a': 1, 'not a native': 1, 'a native speaker': 1, 'native speaker of': 1, 'speaker of english': 1, 'i am worried': 1, 'am worried about': 1, 'worried about this': 1, 'about this business': 1, 'this business memo': 1, 'business memo i': 1, 'memo i am': 1, 'am not good': 1, 'not good at': 1, 'good at business': 1, 'at business writing': 1, 'business writing and': 1, 'writing and i': 1, 'and i need': 1, 'i need to': 1, 'need to make': 1, 'to make sure': 1, 'make sure everything': 1, 'sure everything is': 1, 'everything is concise': 1, 'is concise and': 1, 'concise and clear': 1, 'and clear so': 1, 'clear so that': 1, 'so that i': 1, 'that i don': 1, 'i don t': 1, 'don t get': 1, 't get point': 1, 'get point taken': 1, 'point taken off': 1}) \n",
            "\n",
            "Pre-identified phrases: \n",
            "Counter({'make sure': 1}) \n",
            " \n",
            "\n",
            "['edit', 'check', 'fix', 'correct', 'proofread']\n",
            "['discuss', 'revis', 'rework', 'restructur', 'review', 'collabor', 'analyzerefin', 'rewrit', 'draft', 'outlin', 'plan', 'brainstorm', 'develop', 'improv', 'learn', 'structur'] \n",
            "\n",
            "Not Engaged: \n",
            "\n",
            "Passive Engagement Verbs: []\n",
            "Active Engagement Verbs: [] \n",
            "\n",
            "Less Engaged: \n",
            "\n",
            "Passive Engagement Verbs: ['check', 'fix']\n",
            "Active Engagement Verbs: ['revis'] \n",
            "\n",
            "Neutral: \n",
            "\n",
            "Passive Engagement Verbs: ['proofread', 'edit']\n",
            "Active Engagement Verbs: ['revis'] \n",
            "\n",
            "Engaged: \n",
            "\n",
            "Passive Engagement Verbs: ['fix']\n",
            "Active Engagement Verbs: ['brainstorm'] \n",
            "\n",
            "Very Engaged: \n",
            "\n",
            "Passive Engagement Verbs: []\n",
            "Active Engagement Verbs: ['revis'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem import PorterStemmer\n",
        "from numpy import mean\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#defining functions\n",
        "def clean(string1):\n",
        "    '''removes punctuation and capitalization'''\n",
        "    string1 = str.lower(string1)\n",
        "    string1 = re.sub(r\"[^\\w\\s]|\\d\", r\" \", string1)\n",
        "    string1 = re.sub(r\"  \", r\" \", string1)\n",
        "    return string1\n",
        "\n",
        "def average_len(list1):\n",
        "    '''computes the average length of a list of strings'''\n",
        "    return sum(list1) / len(list1)\n",
        "\n",
        "def num_average(list2):\n",
        "    '''computes the numerical average of a list of integers'''\n",
        "    avg = mean(list2)\n",
        "    return avg \n",
        "\n",
        "def ngram(list1, num):\n",
        "    '''returns n-grams for a given tokenized list'''\n",
        "    n_grams = ngrams(list1, num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "def sentiment_analyzer(appointment_form_text):\n",
        "    '''returns sentiment analysis statistics'''\n",
        "    return analyzer.polarity_scores(appointment_form_text)\n",
        "\n",
        "\n",
        "#retrieving data from Excel\n",
        "fr1 = open(\"/content/Sample Appointment Form Data2.csv\", \"r\", encoding = \"utf-8\")\n",
        "fr = csv.reader(fr1)\n",
        "next(fr)\n",
        "\n",
        "\n",
        "#Preparing the data: clean, tokenize, and lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "all_tokens = [] #list of all words, with repeats\n",
        "all_meaningful_tokens = [] #list of all words without stopwords\n",
        "data = [] #list of words paired with engagement score\n",
        "\n",
        "for row in fr:\n",
        "    clean_strings = clean(row[7])\n",
        "    tokens = word_tokenize(clean_strings)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    for i in lemmatized_tokens:\n",
        "        all_tokens.append(i)\n",
        "    data.append([int(row[4]), lemmatized_tokens]) #[engagement score, [[token1],[token2]]\n",
        "\n",
        "for i in all_tokens:\n",
        "    if i not in stopwords:\n",
        "        all_meaningful_tokens.append(i)\n",
        "\n",
        "print(\"\\n All words: \\n\", sorted(all_tokens), \"\\n\")\n",
        "print(\"All Words Without Stopwords: \\n\", sorted(all_meaningful_tokens), \"\\n\")\n",
        "print(\"Matched Start Scores and Words: \\n\", data, \"\\n\")\n",
        "\n",
        "\n",
        "#BASIC STATISTICS (min, max, avg. string length and frequencies, and sentiment statistics)\n",
        "\n",
        "print(\"\\n \\n BASIC STATISTICS \\n\")\n",
        "print(\"Total words used:\", len(all_tokens))\n",
        "print(\"Total meaningful words used:\", len(all_meaningful_tokens), \"\\n\")\n",
        "\n",
        "lengths = [] #comparing string lengths\n",
        "for i in data: \n",
        "    test = len(i[1])\n",
        "    lengths.append(test)\n",
        "average_length = average_len(lengths)\n",
        "print(\"Average length of strings:\", average_length)\n",
        "print(\"Minimum length of string:\", min(lengths))\n",
        "print(\"Maximum length of string:\", max(lengths))\n",
        "\n",
        "token_freq = Counter(all_tokens) #counting word frequencies with and without stopwords\n",
        "print(\"\\n Number of tokens for each word type:\", token_freq, \"\\n\")\n",
        "meaningful_token_freq = Counter(all_meaningful_tokens)\n",
        "print(\"Number of tokens for each word type (no stopwords):\", meaningful_token_freq, \"\\n\")\n",
        "\n",
        "\n",
        "#Grouping data by engagement score (1-5)\n",
        "not_engaged = []\n",
        "less_engaged = []\n",
        "neutral = []\n",
        "engaged = []\n",
        "very_engaged = []\n",
        "\n",
        "for i in data:\n",
        "    if(i[0] == 1):\n",
        "        not_engaged.append(i)\n",
        "    elif (i[0] == 2):\n",
        "        less_engaged.append(i)\n",
        "    elif (i[0] == 3):\n",
        "        neutral.append(i)\n",
        "    elif (i[0] == 4):\n",
        "        engaged.append(i)\n",
        "    elif (i[0] == 5):\n",
        "        very_engaged.append(i)\n",
        "\n",
        "print(\"Not Engaged:\", not_engaged)\n",
        "print(\"Less Engaged:\", less_engaged)\n",
        "print(\"Neutral:\", neutral)\n",
        "print(\"Engaged:\", engaged)\n",
        "print(\"Very Engaged:\", very_engaged, \"\\n\")\n",
        "\n",
        "not_engaged_lengths = []\n",
        "less_engaged_lengths = []\n",
        "neutral_lengths = []\n",
        "engaged_lengths = []\n",
        "very_engaged_lengths = []\n",
        "\n",
        "engagement_categories = [not_engaged, less_engaged, neutral, engaged, very_engaged]\n",
        "lengths_lists = [not_engaged_lengths, less_engaged_lengths, neutral_lengths, engaged_lengths, very_engaged_lengths]\n",
        "category_names = [\"Not Engaged:\", \"Less Engaged:\", \"Neutral:\", \"Engaged:\", \"Very Engaged:\"]\n",
        "\n",
        "for list in engagement_categories: #calculating average string lengths in each category\n",
        "    for i in list:\n",
        "        test = len(i[1])\n",
        "        index = engagement_categories.index(list)\n",
        "        lengths_lists[index].append(test)\n",
        "    average_length = average_len(lengths_lists[index])\n",
        "    print(\"Average Length of String for\", category_names[index], average_length)\n",
        "\n",
        "#Sentiment Statistics\n",
        "#Analyzing the average intensity of the sentiment (positive, negative, neutral) for appointment form strings in each category\n",
        "print(\"\\n \\n Sentiment Statistics \\n\")\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "ne_sentiments = [] #Creating lists for manipulating the sentiment scores\n",
        "le_sentiments = []\n",
        "n_sentiments = []\n",
        "e_sentiments = []\n",
        "ve_sentiments = []\n",
        "sentiment_lists = [ne_sentiments, le_sentiments, n_sentiments, e_sentiments, ve_sentiments]\n",
        "sentiment_list_names = [\"Not engaged sentiments:\", \"Less engaged sentiments:\", \"Neutral sentiments:\", \"Engaged sentiments:\", \"Very engaged sentiments:\"]\n",
        "\n",
        "fr1.seek(0) #resetting back to beginning of file\n",
        "fr1.readline() #skip line 1 with headings\n",
        "\n",
        "for row in fr: #Adding sentiments to the appropriate category list\n",
        "    sentiments = sentiment_analyzer(row[7])\n",
        "    if int(row[4]) == 1:\n",
        "        ne_sentiments.append(sentiments) \n",
        "    elif int(row[4]) == 2:\n",
        "        le_sentiments.append(sentiments)\n",
        "    elif int(row[4]) == 3:\n",
        "        n_sentiments.append(sentiments)\n",
        "    elif int(row[4]) == 4:\n",
        "        e_sentiments.append(sentiments)\n",
        "    elif int(row[4]) == 5:\n",
        "        ve_sentiments.append(sentiments)\n",
        "\n",
        "for list in sentiment_lists: #averaging the positive, negative, and neutral sentiment scores in each engagement category\n",
        "\n",
        "    index1 = sentiment_lists.index(list)\n",
        "    print(sentiment_list_names[index1], \"\\n\")\n",
        "\n",
        "    negs = []\n",
        "    neus = []\n",
        "    poss = []\n",
        "    compounds = []\n",
        "\n",
        "    for d in list:\n",
        "        negs.append(d['neg'])\n",
        "        neus.append(d['neu'])\n",
        "        poss.append(d['pos'])\n",
        "        compounds.append(d['compound'])\n",
        "    print(\"Negative:\", num_average(negs))\n",
        "    print(\"Neutral:\", num_average(neus))\n",
        "    print(\"Positive:\", num_average(poss))\n",
        "    print(\"Compound:\", num_average(compounds), \"\\n \\n\")\n",
        "\n",
        "\n",
        "#SINGLE WORD CATEGORY ANALYSIS\n",
        "print(\"\\n \\n \\n SINGLE WORD CATEGORY ANALYSIS \\n\")\n",
        "\n",
        "checkbox_categories = [\"evidence\", \"source\", \"idea\", \"organization\", \"structure\", \"convention\", \"transition\", \"flow\", \"argument\", \"vocabulary\", \"tone\", \"style\", \"grammar\", \"mechanic\", \"citation\", \"format\"]\n",
        "verbs = [\"write\", \"edit\", \"help\", \"check\", \"correct\", \"collaborate\", \"discuss\", \"refine\", \"revise\", \"correct\", \"fix\", \"error\", \"mistake\", \"proofread\", \"rewrite\", \"draft\", \"outline\", \"plan\", \"brainstorm\"]\n",
        "\n",
        "for list in engagement_categories:\n",
        "    \n",
        "    index = engagement_categories.index(list)\n",
        "    print(category_names[index])\n",
        "\n",
        "    temp_list = [] #flattening multidimentional lists\n",
        "    for i in list: \n",
        "        temp_list.append(i[1])\n",
        "    flat = [item for sublist in temp_list for item in sublist]\n",
        "\n",
        "    flat2 = [] #deleting stopwords\n",
        "    for word in flat: \n",
        "        if word not in stopwords:\n",
        "            flat2.append(word)\n",
        "    print(sorted(flat2), \"\\n\")\n",
        "\n",
        "    freq = Counter(flat2) #counting word frequencies in each category\n",
        "    print(freq, \"\\n\")\n",
        "\n",
        "    print(\"Checkbox Categories:\") #tallying checkbox focuses in each category\n",
        "    expressed_checkboxes = []\n",
        "    for word in checkbox_categories:\n",
        "        if word in flat2:\n",
        "            expressed_checkboxes.append(word)\n",
        "    print(Counter(expressed_checkboxes), \"\\n\")\n",
        "\n",
        "    print(\"Verbs:\") #tallying verbs in each category\n",
        "    expressed_verbs = []\n",
        "    for word in verbs:\n",
        "        if word in flat2:\n",
        "            expressed_verbs.append(word)\n",
        "    print(Counter(expressed_verbs), \"\\n\")\n",
        "\n",
        "    print(\"Pronouns:\") #tallying pronouns in each category\n",
        "    expressed_pronouns = []\n",
        "    for i in flat:\n",
        "        if i in [\"you\", \"i\", \"we\", \"us\"]:\n",
        "            expressed_pronouns.append(i)\n",
        "    print(Counter(expressed_pronouns), \"\\n \\n\")\n",
        "\n",
        "\n",
        "#PHRASAL N-GRAM CATEGORY ANALYSIS\n",
        "print(\"\\n PHRASAL (N-GRAM) CATEGORY ANALYSIS \\n\")\n",
        "phrases = [\"word choice\", \"just want someone\", \"just want\", \"just wan\", \"look over\", \"make sense\", \"make sure\", \"i need help\", \"can you\", \"can we\", \"could you\", \"could we\", \"i want\", \"i 'm not sure if\", \"please fix\", \"please check\", \"please correct\"]\n",
        "\n",
        "#Bi-gram and tri-gram analysis\n",
        "\n",
        "for list in engagement_categories:\n",
        "\n",
        "    index = engagement_categories.index(list)\n",
        "    print(category_names[index], \"\\n\")\n",
        "\n",
        "    #organic phrases\n",
        "    temp_bigrams = [] #bigram analysis\n",
        "    for i in list:\n",
        "        bigrams = ngram(i[1], 2)\n",
        "        for gram in bigrams:\n",
        "            temp_bigrams.append(gram)\n",
        "    bigram_freq = Counter(temp_bigrams)\n",
        "    print(\"Organic Phrases:\")\n",
        "    print(\"bigrams:\", bigram_freq)\n",
        "\n",
        "    temp_trigrams = [] #trigram analysis\n",
        "    for i in list:\n",
        "        trigrams = ngram(i[1], 3)\n",
        "        for gram in trigrams:\n",
        "            temp_trigrams.append(gram)\n",
        "    trigram_freq = Counter(temp_trigrams)\n",
        "    print(\"trigrams: \", trigram_freq, \"\\n\")\n",
        "\n",
        "    #Pre-identified phrases\n",
        "    expressed_phrases = []\n",
        "    for i in temp_bigrams:\n",
        "        if i in phrases:\n",
        "            expressed_phrases.append(i)\n",
        "    print(\"Pre-identified phrases: \")\n",
        "    print(Counter(expressed_phrases), \"\\n \\n\")\n",
        "\n",
        "#VERB ANALYSIS\n",
        "#MANOVA test will determine whether differences in verb-type usage are statisticalaly significant\n",
        "\n",
        "#stemming data and verb lists for easy identification\n",
        "ps = PorterStemmer()\n",
        "\n",
        "passive_engagement_verbs = [\"edit\", \"check\", \"fix\", \"correct\", \"proofread\"]\n",
        "active_engagement_verbs = [\"discuss\", \"revise\", \"rework\", \"restructure\", \"review\", \"collaborate\", \"analyze\" \"refine\", \"rewrite\", \"draft\", \"outline\", \"plan\", \"brainstorm\", \"develop\", \"improve\", \"learn\", \"structure\"]\n",
        "\n",
        "stemmed_Pverbs = [ps.stem(word) for word in passive_engagement_verbs]\n",
        "stemmed_Averbs = [ps.stem(word) for word in active_engagement_verbs]\n",
        "print(stemmed_Pverbs)\n",
        "print(stemmed_Averbs, \"\\n\")\n",
        "\n",
        "all_stemmed = []\n",
        "data_stemmed = []\n",
        "\n",
        "fr1.seek(0)\n",
        "fr1.readline()\n",
        "for row in fr:\n",
        "    new_line = re.sub(\"wanna\", \"\", row[7]) #removing \"wanna\" from obscuring data\n",
        "    clean_strings = clean(new_line)\n",
        "    tokens = word_tokenize(clean_strings)\n",
        "    stemmed_tokens = [ps.stem(word) for word in tokens]\n",
        "    for i in stemmed_tokens:\n",
        "        all_stemmed.append(i)\n",
        "    data_stemmed.append([int(row[4]), stemmed_tokens])\n",
        "\n",
        "ne_stemmed = []\n",
        "le_stemmed = []\n",
        "n_stemmed = []\n",
        "e_stemmed = []\n",
        "ve_stemmed = []\n",
        "stemmed_lists = [ne_stemmed, le_stemmed, n_stemmed, e_stemmed, ve_stemmed]\n",
        "\n",
        "for i in data_stemmed: #adding stemmed data to the appropriate lists\n",
        "    if(i[0] == 1):\n",
        "        ne_stemmed.append(i)\n",
        "    elif (i[0] == 2):\n",
        "        le_stemmed.append(i)\n",
        "    elif (i[0] == 3):\n",
        "        n_stemmed.append(i)\n",
        "    elif (i[0] == 4):\n",
        "        e_stemmed.append(i)\n",
        "    elif (i[0] == 5):\n",
        "        ve_stemmed.append(i)\n",
        "\n",
        "# Get part of speech tags to ensure above verb forms are actually syntactic verbs\n",
        "for list in stemmed_lists:\n",
        "\n",
        "    index = stemmed_lists.index(list)\n",
        "    print(category_names[index], \"\\n\")\n",
        "\n",
        "    expressed_Pverbs = []\n",
        "    kept_Pverbs = []\n",
        "    expressed_Averbs = []\n",
        "    kept_Averbs = []\n",
        "\n",
        "    for i in list:\n",
        "        tags = nltk.pos_tag(i[1]) #list 'tags' gets rewritten for each engagement category\n",
        "\n",
        "        for i in tags: #if term contains a passive engagement verb, add it to expressed list\n",
        "            if i[0] in stemmed_Pverbs:\n",
        "                expressed_Pverbs.append(i)\n",
        "                if i[1] in [\"VB\", \"VBP\", \"VGB\"]: #if term is not a verb (VB, VBP, VBG), don't add to kept list\n",
        "                    kept_Pverbs.append(i[0])\n",
        "\n",
        "        for i in tags: #if term contains an active engagement verb, add it to expressed list\n",
        "            if i[0] in stemmed_Averbs: \n",
        "                expressed_Averbs.append(i)\n",
        "                if i[1] in [\"VB\", \"VBP\", \"VBG\"]: #if term is not a verb (VB, VBP, VBG), don't add to kept list\n",
        "                    kept_Averbs.append(i[0])\n",
        "    \n",
        "    print(\"Passive Engagement Verbs:\", kept_Pverbs)\n",
        "    print(\"Active Engagement Verbs:\", kept_Averbs, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "fr1.close()"
      ]
    }
  ]
}